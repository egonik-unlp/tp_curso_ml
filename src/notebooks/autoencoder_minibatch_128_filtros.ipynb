{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "autoencoder_minibatch.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/egonik-unlp/tp_curso_ml/blob/main/src/notebooks/autoencoder_minibatch_128_filtros.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytMhZKgfLD9e",
        "outputId": "42fa04bc-9abb-4882-8305-c3486b6d49ac"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import requests\n",
        "import json\n",
        "import os\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import normalize, MinMaxScaler\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy-iCvj0WYbG"
      },
      "source": [
        "codigo_numerico=requests.get('https://raw.githubusercontent.com/egonik-unlp/tp_curso_ml/d61d2cd55fe79750c892a739129de1e02ead2d29/src/pickle/json_files/codigo_blosum62.json').json()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPGvGWF3M5ua",
        "outputId": "095106bc-5b17-4cb1-8142-817948cbcafd"
      },
      "source": [
        "%cd /content/drive/MyDrive/protein_classifier_with_locations/autoencoder/dataset\n",
        "proteome=np.load(\"proteome_short.npy\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/protein_classifier_with_locations/autoencoder/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1-4wMEEQZtn"
      },
      "source": [
        "X_train_val, X_test=train_test_split(proteome)\n",
        "X_train, X_valid=train_test_split(X_train_val)\n",
        "\n",
        "# Por el momento voy a tratar de escalar manualmente, para evitar problemas de redondeo en la\n",
        "# conversi√≥n inversa\n",
        "\n",
        "X_train_n=X_train/24 # el valor maximo del 'codigo numerico'\n",
        "X_valid_n=X_valid/24\n",
        "X_test_n=X_test/24"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_DLMOxY--ex"
      },
      "source": [
        "# Reshapeo para que funcione en el AE\n",
        "X_train_bis=X_train_n.reshape(-1,2000,1)\n",
        "X_valid_bis=X_valid_n.reshape(-1,2000,1)\n",
        "X_test_bis=X_test_n.reshape(-1,2000,1)\n",
        "del X_train_n\n",
        "del X_valid_n\n",
        "del X_test_n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbDj1bLzNqLF",
        "outputId": "dbbbc7db-c676-4506-94c0-50b2dfe84891"
      },
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "conv_encoder = keras.models.Sequential([\n",
        "keras.layers.Reshape([2000, 1], input_shape=[2000,1]),\n",
        "keras.layers.Conv1D(16, kernel_size=3, padding=\"same\", activation=\"selu\"),\n",
        "keras.layers.MaxPool1D(pool_size=2),\n",
        "keras.layers.Conv1D(32, kernel_size=3, padding=\"same\", activation=\"selu\"),\n",
        "keras.layers.MaxPool1D(pool_size=2),\n",
        "keras.layers.Conv1D(64, kernel_size=3, padding=\"same\", activation=\"selu\"),\n",
        "keras.layers.MaxPool1D(pool_size=2),\n",
        "keras.layers.Conv1D(128, kernel_size=3, padding='same', activation=\"selu\"),\n",
        "keras.layers.MaxPool1D(pool_size=2)\n",
        "])\n",
        "\n",
        "\n",
        "conv_decoder = keras.models.Sequential([\n",
        "keras.layers.Conv1DTranspose(64, kernel_size=3,strides=2,padding='same',\n",
        "                             input_shape=[125,128]),\n",
        "\n",
        "keras.layers.Conv1DTranspose(32, kernel_size=3, strides=2, padding=\"same\",\n",
        "activation=\"selu\",\n",
        "#input_shape=[250, 64]\n",
        "),\n",
        "keras.layers.Conv1DTranspose(16, kernel_size=3, strides=2, padding=\"same\",\n",
        "activation=\"selu\"),\n",
        "keras.layers.Conv1DTranspose(1, kernel_size=3, strides=2, padding=\"same\",\n",
        "activation=\"sigmoid\"),\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "conv_ae = keras.models.Sequential([conv_encoder, conv_decoder])\n",
        "conv_ae.summary()\n",
        "datetime_string=datetime.now().strftime(\"%d-%m_%H-%M\")\n",
        "folder=dir='/content/drive/MyDrive/protein_classifier_with_locations/autoencoder/sequences_checkpoints/runs/{}'.format(datetime_string)\n",
        "os.mkdir(folder)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential (Sequential)      (None, 250, 64)           7840      \n",
            "_________________________________________________________________\n",
            "sequential_1 (Sequential)    (None, 2000, 1)           7777      \n",
            "=================================================================\n",
            "Total params: 15,617\n",
            "Trainable params: 15,617\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dpHqdFXpbaKw"
      },
      "source": [
        "nums_codes={v:k for k,v in codigo_numerico.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWGyEt9ScWlb"
      },
      "source": [
        "def unpad(query, target):\n",
        "  try:\n",
        "    idx=np.where(query==0)[0][-1] + 1\n",
        "  except IndexError:\n",
        "    idx=0\n",
        "  return query[idx:], target[idx:]\n",
        "class CustomSaver(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if epoch %50 == 0:  # or save after some epoch, each k-th epoch etc.\n",
        "      self.model.save('{}/autoencoder_prot_epoch_{}.h5'.format(folder,epoch))\n",
        "cs=CustomSaver()\n",
        "  \n",
        "class ProteinEvaluator(keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}, protein_code=nums_codes,folder=folder):\n",
        "    if epoch%5 == 0 and epoch!=0:\n",
        "      predictions=self.model.predict(X_valid_bis)\n",
        "      pred_seqs=[]\n",
        "      for n,(query, target) in enumerate(zip(X_valid_bis.reshape(-1,2000), predictions.reshape(-1,2000))):\n",
        "        qry,tgt=unpad(query,target)\n",
        "        del qry\n",
        "        pred_seqs.append(tgt)\n",
        "      compressed_proteins=[''.join(np.vectorize(lambda x:nums_codes.get(round(x*25),'X'))(seq)) for seq in pred_seqs]\n",
        "      with open('{}/pred_seqs_epoch_{}.json'.format(folder,epoch), 'w') as file:\n",
        "        json.dump(compressed_proteins, file)\n",
        "# pe=ProteinEvaluator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-VWgQHklCXZ"
      },
      "source": [
        "np.save('{}/X_valid_bis'.format(folder), X_valid_bis)\n",
        "np.save('{}/X_test_bis'.format(folder), X_test_bis)\n",
        "np.save('{}/X_train_bis'.format(folder), X_train_bis)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57bJtD7Q8yCd",
        "outputId": "14ef20ce-b0d1-4e83-d0da-9d42a379356b"
      },
      "source": [
        "conv_ae.compile(loss=\"binary_crossentropy\",\n",
        "# optimizer=keras.optimizers.SGD(learning_rate=1.5))\n",
        "optimizer=keras.optimizers.Adam())\n",
        "history = conv_ae.fit(X_train_bis, X_train_bis, epochs=1000,\n",
        "  validation_data=(X_valid_bis, X_valid_bis), callbacks=[cs], batch_size=256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/1000\n",
            "  6/373 [..............................] - ETA: 31s - loss: 0.6705WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0356s vs `on_train_batch_end` time: 0.0425s). Check your callbacks.\n",
            "373/373 [==============================] - 81s 67ms/step - loss: 0.1446 - val_loss: 0.1161\n",
            "Epoch 2/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.1124 - val_loss: 0.1085\n",
            "Epoch 3/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.1062 - val_loss: 0.1043\n",
            "Epoch 4/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.1036 - val_loss: 0.1027\n",
            "Epoch 5/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.1026 - val_loss: 0.1019\n",
            "Epoch 6/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.1019 - val_loss: 0.1014\n",
            "Epoch 7/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.1014 - val_loss: 0.1009\n",
            "Epoch 8/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.1009 - val_loss: 0.1005\n",
            "Epoch 9/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.1006 - val_loss: 0.1002\n",
            "Epoch 10/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.1003 - val_loss: 0.1000\n",
            "Epoch 11/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.1001 - val_loss: 0.0998\n",
            "Epoch 12/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0999 - val_loss: 0.0997\n",
            "Epoch 13/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0998 - val_loss: 0.0995\n",
            "Epoch 14/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0997 - val_loss: 0.0993\n",
            "Epoch 15/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0996 - val_loss: 0.0993\n",
            "Epoch 16/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0995 - val_loss: 0.0992\n",
            "Epoch 17/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0994 - val_loss: 0.0992\n",
            "Epoch 18/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0994 - val_loss: 0.0991\n",
            "Epoch 19/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0993 - val_loss: 0.0990\n",
            "Epoch 20/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0992 - val_loss: 0.0989\n",
            "Epoch 21/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0992 - val_loss: 0.0988\n",
            "Epoch 22/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0991 - val_loss: 0.0988\n",
            "Epoch 23/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0991 - val_loss: 0.0989\n",
            "Epoch 24/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0991 - val_loss: 0.0987\n",
            "Epoch 25/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0990 - val_loss: 0.0987\n",
            "Epoch 26/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0990 - val_loss: 0.0988\n",
            "Epoch 27/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0990 - val_loss: 0.0988\n",
            "Epoch 28/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0989 - val_loss: 0.0987\n",
            "Epoch 29/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0989 - val_loss: 0.0986\n",
            "Epoch 30/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0989 - val_loss: 0.0986\n",
            "Epoch 31/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0989 - val_loss: 0.0986\n",
            "Epoch 32/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0988 - val_loss: 0.0986\n",
            "Epoch 33/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0988 - val_loss: 0.0985\n",
            "Epoch 34/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0988 - val_loss: 0.0985\n",
            "Epoch 35/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0988 - val_loss: 0.0985\n",
            "Epoch 36/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0988 - val_loss: 0.0985\n",
            "Epoch 37/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0987 - val_loss: 0.0984\n",
            "Epoch 38/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0987 - val_loss: 0.0984\n",
            "Epoch 39/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0987 - val_loss: 0.0984\n",
            "Epoch 40/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0987 - val_loss: 0.0984\n",
            "Epoch 41/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0987 - val_loss: 0.0984\n",
            "Epoch 42/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0987 - val_loss: 0.0985\n",
            "Epoch 43/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0987 - val_loss: 0.0985\n",
            "Epoch 44/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0986 - val_loss: 0.0984\n",
            "Epoch 45/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0986 - val_loss: 0.0984\n",
            "Epoch 46/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0986 - val_loss: 0.0983\n",
            "Epoch 47/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0986 - val_loss: 0.0983\n",
            "Epoch 48/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0986 - val_loss: 0.0983\n",
            "Epoch 49/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0986 - val_loss: 0.0983\n",
            "Epoch 50/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0986 - val_loss: 0.0983\n",
            "Epoch 51/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0986 - val_loss: 0.0983\n",
            "Epoch 52/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0986 - val_loss: 0.0982\n",
            "Epoch 53/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0986 - val_loss: 0.0983\n",
            "Epoch 54/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0985 - val_loss: 0.0983\n",
            "Epoch 55/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0985 - val_loss: 0.0983\n",
            "Epoch 56/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0985 - val_loss: 0.0982\n",
            "Epoch 57/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0985 - val_loss: 0.0982\n",
            "Epoch 58/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0985 - val_loss: 0.0982\n",
            "Epoch 59/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0985 - val_loss: 0.0983\n",
            "Epoch 60/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0985 - val_loss: 0.0982\n",
            "Epoch 61/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0985 - val_loss: 0.0983\n",
            "Epoch 62/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0985 - val_loss: 0.0983\n",
            "Epoch 63/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0985 - val_loss: 0.0982\n",
            "Epoch 64/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0985 - val_loss: 0.0982\n",
            "Epoch 65/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0985 - val_loss: 0.0982\n",
            "Epoch 66/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0985 - val_loss: 0.0982\n",
            "Epoch 67/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0985 - val_loss: 0.0981\n",
            "Epoch 68/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0985 - val_loss: 0.0981\n",
            "Epoch 69/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0982\n",
            "Epoch 70/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 71/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 72/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 73/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0982\n",
            "Epoch 74/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 75/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 76/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 77/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 78/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 79/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 80/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 81/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 82/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0985\n",
            "Epoch 83/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 84/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 85/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 86/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 87/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 88/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0982\n",
            "Epoch 89/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 90/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0984 - val_loss: 0.0981\n",
            "Epoch 91/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0981\n",
            "Epoch 92/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0981\n",
            "Epoch 93/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0981\n",
            "Epoch 94/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0981\n",
            "Epoch 95/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 96/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0982\n",
            "Epoch 97/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0981\n",
            "Epoch 98/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 99/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 100/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 101/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 102/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 103/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0981\n",
            "Epoch 104/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 105/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 106/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 107/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 108/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 109/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0981\n",
            "Epoch 110/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 111/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 112/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 113/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 114/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 115/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 116/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0982\n",
            "Epoch 117/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 118/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 119/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 120/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 121/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 122/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 123/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 124/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0981\n",
            "Epoch 125/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0980\n",
            "Epoch 126/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 127/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0983 - val_loss: 0.0979\n",
            "Epoch 128/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 129/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0982 - val_loss: 0.0980\n",
            "Epoch 130/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0980\n",
            "Epoch 131/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0980\n",
            "Epoch 132/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 133/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0980\n",
            "Epoch 134/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 135/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 136/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 137/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0982 - val_loss: 0.0980\n",
            "Epoch 138/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 139/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 140/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 141/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0980\n",
            "Epoch 142/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 143/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 144/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 145/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 146/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 147/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0980\n",
            "Epoch 148/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 149/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 150/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 151/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 152/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 153/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 154/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 155/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 156/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 157/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 158/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 159/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0980\n",
            "Epoch 160/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 161/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 162/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 163/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 164/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 165/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 166/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 167/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 168/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 169/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 170/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 171/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 172/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 173/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 174/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 175/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 176/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 177/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0978\n",
            "Epoch 178/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 179/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 180/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 181/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0978\n",
            "Epoch 182/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 183/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0980\n",
            "Epoch 184/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0980\n",
            "Epoch 185/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 186/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 187/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 188/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0982 - val_loss: 0.0979\n",
            "Epoch 189/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 190/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 191/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 192/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 193/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 194/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 195/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 196/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 197/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 198/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 199/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 200/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 201/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 202/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 203/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 204/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 205/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 206/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 207/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 208/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 209/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 210/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 211/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 212/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0980\n",
            "Epoch 213/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 214/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 215/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 216/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 217/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 218/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 219/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0981\n",
            "Epoch 220/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 221/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 222/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 223/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 224/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 225/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 226/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 227/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 228/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 229/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 230/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 231/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 232/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 233/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 234/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 235/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 236/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 237/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 238/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0980\n",
            "Epoch 239/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 240/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 241/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 242/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 243/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 244/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 245/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 246/1000\n",
            "373/373 [==============================] - 23s 61ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 247/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 248/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 249/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 250/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 251/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 252/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 253/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 254/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 255/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 256/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 257/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 258/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 259/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 260/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 261/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 262/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 263/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 264/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 265/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 266/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 267/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 268/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 269/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 270/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 271/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 272/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 273/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 274/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 275/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 276/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 277/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 278/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 279/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 280/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 281/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 282/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 283/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0981 - val_loss: 0.0977\n",
            "Epoch 284/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 285/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 286/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 287/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0979\n",
            "Epoch 288/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 289/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 290/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 291/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 292/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 293/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 294/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0981 - val_loss: 0.0977\n",
            "Epoch 295/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0981 - val_loss: 0.0977\n",
            "Epoch 296/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 297/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0980 - val_loss: 0.0978\n",
            "Epoch 298/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 299/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0981 - val_loss: 0.0978\n",
            "Epoch 300/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0981 - val_loss: 0.0977\n",
            "Epoch 301/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0981 - val_loss: 0.0977\n",
            "Epoch 302/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0980 - val_loss: 0.0978\n",
            "Epoch 303/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 304/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 305/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 306/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 307/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 308/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0980 - val_loss: 0.0978\n",
            "Epoch 309/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 310/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 311/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 312/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0979\n",
            "Epoch 313/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 314/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0978\n",
            "Epoch 315/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 316/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 317/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 318/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 319/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 320/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0978\n",
            "Epoch 321/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 322/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 323/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 324/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 325/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0978\n",
            "Epoch 326/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 327/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0980 - val_loss: 0.0979\n",
            "Epoch 328/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 329/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 330/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 331/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 332/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0980 - val_loss: 0.0978\n",
            "Epoch 333/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0978\n",
            "Epoch 334/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 335/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 336/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 337/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 338/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 339/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 340/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 341/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 342/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 343/1000\n",
            "373/373 [==============================] - 23s 62ms/step - loss: 0.0980 - val_loss: 0.0979\n",
            "Epoch 344/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 345/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 346/1000\n",
            "373/373 [==============================] - 24s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 347/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0978\n",
            "Epoch 348/1000\n",
            "373/373 [==============================] - 23s 63ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 349/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 350/1000\n",
            "373/373 [==============================] - 24s 65ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 351/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 352/1000\n",
            "373/373 [==============================] - 24s 65ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 353/1000\n",
            "373/373 [==============================] - 24s 65ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 354/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 355/1000\n",
            "373/373 [==============================] - 24s 65ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 356/1000\n",
            "373/373 [==============================] - 24s 65ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 357/1000\n",
            "373/373 [==============================] - 24s 65ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 358/1000\n",
            "373/373 [==============================] - 24s 65ms/step - loss: 0.0980 - val_loss: 0.0979\n",
            "Epoch 359/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 360/1000\n",
            "373/373 [==============================] - 24s 64ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 361/1000\n",
            "373/373 [==============================] - 26s 71ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 362/1000\n",
            "373/373 [==============================] - 24s 65ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 363/1000\n",
            "373/373 [==============================] - 24s 66ms/step - loss: 0.0980 - val_loss: 0.0978\n",
            "Epoch 364/1000\n",
            "373/373 [==============================] - 24s 65ms/step - loss: 0.0980 - val_loss: 0.0977\n",
            "Epoch 365/1000\n",
            "227/373 [=================>............] - ETA: 8s - loss: 0.0982"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0gV1d6WHYhm"
      },
      "source": [
        "original_seqs=[seq for seq in X_test_bis.reshape(-1,2000)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIYgyfET3bgc"
      },
      "source": [
        "# nums_codes={v:k for k,v in codigo_numerico.items()} \n",
        "# # compressed_proteins=[''.join(np.vectorize(lambda x:nums_codes.get(round(x*25),'X'))(seq)) for seq in pred_seqs]\n",
        "# original_proteins=[''.join(np.vectorize(lambda x:nums_codes.get(round(x*25),'X'))(seq)) for seq in original_seqs]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhHuHfIhyEUn"
      },
      "source": [
        "# np.save('/content/drive/MyDrive/protein_classifier_with_locations/autoencoder/predictions',predictions)\n",
        "# np.save('/content/drive/MyDrive/protein_classifier_with_locations/autoencoder/compressed_proteins',compressed_proteins)\n",
        "# np.save('/content/drive/MyDrive/protein_classifier_with_locations/autoencoder/original_proteins', original_proteins)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaYcKEFD0V2b"
      },
      "source": [
        "conv_ae.save(\"{}/model.h5\".format(folder))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqAzkyJ8GySI"
      },
      "source": [
        "pd.DataFrame(history.history).to_excel(\"{}/history.xlsx\".format(folder))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}